{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from lxml import etree\n",
    "from collections import Counter\n",
    "import camelot\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/MetaDataOverviewReport.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-20eb97a262c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../data/MetaDataOverviewReport.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'latin-1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/MetaDataOverviewReport.csv'"
     ]
    }
   ],
   "source": [
    "## Converting pdf's to text\n",
    "\n",
    "import csv\n",
    "\n",
    "reader = csv.DictReader(open('../data/MetaDataOverviewReport.csv', encoding='latin-1'))\n",
    "count = 0\n",
    "\n",
    "for line in reader:\n",
    "    pdfurl = line[' PdfUrl']\n",
    "    filename = pdfurl.split('/')[-1]\n",
    "    pdftitle = (line['WobFile'] + '_' + filename).lower()\n",
    "    pdftitlestripped = pdftitle.strip('.pdf')\n",
    "    txturl = '../data/txt/' + pdftitle.strip('.pdf') + '.txt'\n",
    "    \n",
    "    ! curl -s {pdfurl} > {pdftitle} ; pdftk {pdftitle} burst output {pdftitlestripped}-%d.pdf ; rm {pdftitle}\n",
    "    ! for file in *.pdf ; do pdftotext \"$file\" ; done ; rm *.pdf ; rm doc_data.txt\n",
    "        \n",
    "    count+= 1\n",
    "    print(count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocrdata = pd.read_csv('../data/covid19wob_files_df.tar.xz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_split(text):\n",
    "    \n",
    "    \n",
    "    splitted = re.split('\\W+', text)\n",
    "    cleaned = [word.lower() for word in splitted if word]\n",
    "     \n",
    "    return cleaned\n",
    "\n",
    "def count_words(text):\n",
    "    \n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "\n",
    "    corpus = clean_split(text)\n",
    "    \n",
    "    return len(corpus)\n",
    "\n",
    "def count_characters(text):\n",
    "    \n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "    \n",
    "    corpus = clean_split(text)\n",
    "    \n",
    "    return sum([len(word) for word in corpus])\n",
    "\n",
    "def create_bow(text):\n",
    "    \n",
    "    if not isinstance(text, str):\n",
    "        return {}\n",
    "    \n",
    "    cleaned = clean_split(text)\n",
    "    \n",
    "    return Counter(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocrdata['nr_words'] = ocrdata['text'].map(lambda x: count_words(x))\n",
    "ocrdata['nr_chars'] = ocrdata['text'].map(lambda x: count_characters(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bows_ocr = {}\n",
    "\n",
    "for index, row in ocrdata[:-1].iterrows():\n",
    "    bows_ocr[str(row['name']) + '-' + str(int(row['page']))] = create_bow(row['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd905be8906041a890ceff4260b6c0ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=33905.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dirpath = '../data/txt/txt//'\n",
    "\n",
    "bows = {}\n",
    "for txtfile in tqdm(os.listdir(dirpath)):\n",
    "\n",
    "    contents = open(dirpath+txtfile, encoding='utf-8').read()\n",
    "\n",
    "    bows[txtfile.strip('.txt')] = create_bow(contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bows_ocr = {key: bows_ocr[key] for key in bows_ocr if key in bows}\n",
    "bows = {key: bows[key] for key in bows if key in bows_ocr}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's compare!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "28331 28331\n"
     ]
    }
   ],
   "source": [
    "# Should be an empty set!\n",
    "print(set(bows_ocr) - set(bows))\n",
    "\n",
    "print(len(bows_ocr), len(bows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages with no found words (pdf to text): 6586 (23.25%)\n",
      "Pages with no found words (ocr): 121 (0.43%)\n"
     ]
    }
   ],
   "source": [
    "empty_bags = [bag for bag in bows if len(bows[bag]) == 0]\n",
    "empty_bags_ocr = [bag for bag in bows_ocr if len(bows_ocr[bag]) == 0]\n",
    "\n",
    "print(f'Pages with no found words (pdf to text): {len(empty_bags)} ({round(len(empty_bags) / len(bows) * 100, 2)}%)')\n",
    "print(f'Pages with no found words (ocr): {len(empty_bags_ocr)} ({round(len(empty_bags_ocr) / len(bows_ocr) * 100, 2)}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many increases/decreases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increases\n",
    "increases = {}\n",
    "\n",
    "for page in bows:\n",
    "    \n",
    "    original = sum(bows[page].values())\n",
    "    ocr = sum(bows_ocr[page].values())\n",
    "    \n",
    "    increases[page] = ocr - original\n",
    "    \n",
    "increases_ocr = pd.Series(increases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreases: 10728 (37.87%)\n",
      "Increases: 13798 (48.7%)\n",
      "Same amount: 3805 (13.43%)\n",
      "Highest increase: 2021. Biggest decrease: -1097\n",
      "Mean increase: 47.5712470438742\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "e894904cd8de2892fa1143092d638832_bijlage-3-bij-besluit-op-wob-verzoek-over-contacten-bedrijfsleven-coronasteunmaatregelen-41   -1097\n",
       "5d03dc8b8621ea5e53ad6fe7b44c8533_wob-verzoek-inz-coronamaatregelen-op-scholen-16-37                                            -1029\n",
       "5d03dc8b8621ea5e53ad6fe7b44c8533_wob-verzoek-inz-coronamaatregelen-op-scholen-16-33                                            -1014\n",
       "e894904cd8de2892fa1143092d638832_bijlage-2-bij-besluit-op-wob-verzoek-over-contacten-bedrijfsleven-coronasteunmaatregelen-1     -978\n",
       "5d03dc8b8621ea5e53ad6fe7b44c8533_wob-verzoek-inz-coronamaatregelen-op-scholen-14-34                                             -946\n",
       "                                                                                                                                ... \n",
       "40f5564f839324b9af20c295dd261007_wob-documenten-78                                                                              1334\n",
       "8a676a3415d986008b02572cb9310894_besluit-covid-19-suriname-inclusief-inventarisatielijst-11                                     1488\n",
       "51a2c7e46846e5d0f5b66f11f627cea9_samengevoegde-documenten-sierteeltsector-geredigeerd-deel-1-132                                1562\n",
       "b752f2f8f1fc2db6fe146785b4b521a3_03-iao-april-668                                                                               1677\n",
       "855c22f53e49d18948f4620d8eb7fba7_bijlage-1-bij-besluit-op-wob-verzoek-over-reisadvies-voor-china-door-covid-19-60               2021\n",
       "Length: 28331, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Decreases: {(increases_ocr < 0).sum()} ({round((increases_ocr < 0).mean() * 100, 2)}%)\")\n",
    "print(f\"Increases: {(increases_ocr > 0).sum()} ({round((increases_ocr > 0).mean() * 100, 2)}%)\")\n",
    "print(f\"Same amount: {(increases_ocr == 0).sum()} ({round((increases_ocr == 0).mean() * 100, 2)}%)\")\n",
    "\n",
    "print(f\"Highest increase: {increases_ocr.max()}. Biggest decrease: {increases_ocr.min()}\")\n",
    "print(f\"Mean increase: {increases_ocr.mean()}\")\n",
    "\n",
    "increases_ocr.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = {}\n",
    "\n",
    "for page in bows:\n",
    "    \n",
    "    subsets[page] = set(bows[page].keys()).issubset(set(bows_ocr[page].keys()))\n",
    "    \n",
    "subsets = pd.Series(subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9204 (32.49%) docs are a subset of the ocr-ed doc\n"
     ]
    }
   ],
   "source": [
    "print(f\"{subsets.sum()} ({round(subsets.sum() / subsets.count() * 100, 2)}%) docs are a subset of the ocr-ed doc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dutch_words\n",
    "\n",
    "words = dutch_words.get_ranked()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4dbe839375447f48b8db48efa0dfe3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=28331.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dutch_words_counter = {}\n",
    "\n",
    "for doc in tqdm(bows):\n",
    "    \n",
    "    ocr_dutch_words = sum([bows_ocr[doc][word] for word in bows_ocr[doc] if word in words])\n",
    "    pdftotext_dutch_words = sum([bows[doc][word] for word in bows[doc] if word in words])\n",
    "\n",
    "    dutch_words_counter[doc] = {'pdftotext': pdftotext_dutch_words, 'ocr': ocr_dutch_words}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dutch_words_df = pd.DataFrame(dutch_words_counter).T\n",
    "dutch_words_df['gain'] = dutch_words_df['ocr'] - dutch_words_df['pdftotext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A total of 1057439 extra Dutch words are identified after OCR!\n",
      "That's a mean gain of 37.32 words per document\n",
      "21822 (77.03%) documents with a gain of >= 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"A total of {dutch_words_df.gain.sum()} extra Dutch words are identified after OCR!\")\n",
    "print(f\"That's a mean gain of {round(dutch_words_df.gain.mean(), 2)} words per document\")\n",
    "print(f\"{sum(dutch_words_df.gain >= 0)} ({round(sum(dutch_words_df.gain >= 0) / len(bows) * 100, 2)}%) documents with a gain of >= 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
