{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from lxml import etree\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocrdata = pd.read_csv('../data/covid19wob_files_df.tar.xz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_split(text):\n",
    "    \n",
    "    \n",
    "    splitted = re.split('\\W+', text)\n",
    "    cleaned = [word.lower() for word in splitted if word]\n",
    "     \n",
    "    return cleaned\n",
    "\n",
    "def count_words(text):\n",
    "    \n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "\n",
    "    corpus = clean_split(text)\n",
    "    \n",
    "    return len(corpus)\n",
    "\n",
    "def count_characters(text):\n",
    "    \n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "    \n",
    "    corpus = clean_split(text)\n",
    "    \n",
    "    return sum([len(word) for word in corpus])\n",
    "\n",
    "def create_bow(text):\n",
    "    \n",
    "    if not isinstance(text, str):\n",
    "        return {}\n",
    "    \n",
    "    cleaned = clean_split(text)\n",
    "    \n",
    "    return Counter(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocrdata['nr_words'] = ocrdata['text'].map(lambda x: count_words(x))\n",
    "ocrdata['nr_chars'] = ocrdata['text'].map(lambda x: count_characters(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bows_ocr = {}\n",
    "\n",
    "for index, row in ocrdata[:-1].iterrows():\n",
    "    bows_ocr[str(row['name']) + '-' + str(int(row['page']))] = create_bow(row['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "dirpath = '../data/XMLs//'\n",
    "\n",
    "bows = {}\n",
    "for xmlfile in os.listdir(dirpath):\n",
    "\n",
    "    root = etree.parse(dirpath+xmlfile)\n",
    "    pages = root.xpath('//page')\n",
    "\n",
    "    for pagenumber, page in enumerate(pages):\n",
    "\n",
    "        bows[xmlfile.strip('.xml') + '-' + str(pagenumber+1)] = create_bow(' '.join(page.xpath('text/text()')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "bows_ocr = {key: bows_ocr[key] for key in bows_ocr if key in bows}\n",
    "bows = {key: bows[key] for key in bows if key in bows_ocr}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's compare!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "30881 30881\n"
     ]
    }
   ],
   "source": [
    "# Should be an empty set!\n",
    "print(set(bows_ocr) - set(bows))\n",
    "\n",
    "print(len(bows_ocr), len(bows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages with no found words (pdf to text): 7550 (24.45%)\n",
      "Pages with no found words (ocr): 345 (1.12%)\n"
     ]
    }
   ],
   "source": [
    "empty_bags = [bag for bag in bows if len(bows[bag]) == 0]\n",
    "empty_bags_ocr = [bag for bag in bows_ocr if len(bows_ocr[bag]) == 0]\n",
    "\n",
    "print(f'Pages with no found words (pdf to text): {len(empty_bags)} ({round(len(empty_bags) / len(bows) * 100, 2)}%)')\n",
    "print(f'Pages with no found words (ocr): {len(empty_bags_ocr)} ({round(len(empty_bags_ocr) / len(bows_ocr) * 100, 2)}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many increases/decreases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increases\n",
    "increases = {}\n",
    "\n",
    "for page in bows:\n",
    "    \n",
    "    original = sum(bows[page].values())\n",
    "    ocr = sum(bows_ocr[page].values())\n",
    "    \n",
    "    increases[page] = ocr - original\n",
    "    \n",
    "increases_ocr = pd.Series(increases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreases: 10559 (34.19%)\n",
      "Increases: 16442 (53.24%)\n",
      "Same amount: 3880 (12.56%)\n",
      "Highest increase: 2021. Biggest decrease: -2310\n",
      "Mean increase: 57.8344613192578\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "d8d9c5015c9ceb952052f29e1a27ed1f_inventarislijst-deel-1-1                                                           -2310\n",
       "3fa52482438ed25b69b0be300baaf3c0_documenten-292                                                                     -1245\n",
       "d8d9c5015c9ceb952052f29e1a27ed1f_inventarislijst-deel-2-1                                                           -1198\n",
       "3fa52482438ed25b69b0be300baaf3c0_documenten-276                                                                      -936\n",
       "3fa52482438ed25b69b0be300baaf3c0_documenten-313                                                                      -930\n",
       "                                                                                                                     ... \n",
       "40f5564f839324b9af20c295dd261007_wob-documenten-79                                                                   1255\n",
       "40f5564f839324b9af20c295dd261007_wob-documenten-78                                                                   1334\n",
       "8a676a3415d986008b02572cb9310894_besluit-covid-19-suriname-inclusief-inventarisatielijst-11                          1488\n",
       "51a2c7e46846e5d0f5b66f11f627cea9_samengevoegde-documenten-sierteeltsector-geredigeerd-deel-1-132                     1562\n",
       "855c22f53e49d18948f4620d8eb7fba7_bijlage-1-bij-besluit-op-wob-verzoek-over-reisadvies-voor-china-door-covid-19-60    2021\n",
       "Length: 30881, dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Decreases: {(increases_ocr < 0).sum()} ({round((increases_ocr < 0).mean() * 100, 2)}%)\")\n",
    "print(f\"Increases: {(increases_ocr > 0).sum()} ({round((increases_ocr > 0).mean() * 100, 2)}%)\")\n",
    "print(f\"Same amount: {(increases_ocr == 0).sum()} ({round((increases_ocr == 0).mean() * 100, 2)}%)\")\n",
    "\n",
    "print(f\"Highest increase: {increases_ocr.max()}. Biggest decrease: {increases_ocr.min()}\")\n",
    "print(f\"Mean increase: {increases_ocr.mean()}\")\n",
    "\n",
    "increases_ocr.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('n', 165), ('e', 113), ('i', 96), ('l', 67), ('t', 60), ('m', 47), ('de', 45), ('d', 39), ('a', 35), ('k', 31)]\n",
      "[('de', 31), ('van', 20), ('voor', 17), ('en', 12), ('in', 10), ('te', 10), ('dit', 9), ('het', 8), ('e', 7), ('een', 6)]\n"
     ]
    }
   ],
   "source": [
    "# Probleem: pdf to text gaat niet altijd lekker... Verklaart veel van de decreases\n",
    "# Een decrease is dus eigenlijk vaak juist goed...\n",
    "print(bows['3fa52482438ed25b69b0be300baaf3c0_documenten-276'].most_common(10))\n",
    "print(bows_ocr['3fa52482438ed25b69b0be300baaf3c0_documenten-276'].most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = {}\n",
    "\n",
    "for page in bows:\n",
    "    \n",
    "    subsets[page] = set(bows[page].keys()).issubset(set(bows_ocr[page].keys()))\n",
    "    \n",
    "subsets = pd.Series(subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10705 (34.67%) docs are a subset of the ocr-ed doc\n"
     ]
    }
   ],
   "source": [
    "print(f\"{subsets.sum()} ({round(subsets.sum() / subsets.count() * 100, 2)}%) docs are a subset of the ocr-ed doc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
